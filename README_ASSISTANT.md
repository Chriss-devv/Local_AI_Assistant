# ğŸ¤– AI Assistant v7.82

Un asistente de IA personalizable con bÃºsqueda web contextual, enriquecimiento inteligente de queries y auto-guardado.

![Version](https://img.shields.io/badge/version-7.82-blue)
![Python](https://img.shields.io/badge/python-3.8+-green)
![License](https://img.shields.io/badge/license-MIT-orange)

---

## âœ¨ CaracterÃ­sticas

### ğŸ¯ PersonalizaciÃ³n Completa
- **Wizard de configuraciÃ³n** en primera ejecuciÃ³n
- Nombre personalizable del asistente y usuario
- Rol y personalidad customizable
- Directorio de logs configurable
- Intervalo de auto-guardado ajustable

### ğŸŒ BÃºsqueda Web Contextual (v7.82)
- **Enriquecimiento inteligente**: "busca mejorarlo" â†’ "busca mejorar cÃ³digo Python"
- **Contexto conversacional**: Incluye Ãºltimos 3 intercambios automÃ¡ticamente
- **Queries optimizadas**: Detecta referencias vagas y las enriquece
- **ImplementaciÃ³n directa**: Genera cÃ³digo basÃ¡ndose en bÃºsquedas

### ğŸ“… Conciencia Temporal
- Sabe quÃ© dÃ­a es HOY, AYER y MAÃ‘ANA
- Prioriza informaciÃ³n del aÃ±o actual (2026)
- Solicita bÃºsquedas web para eventos recientes

### ğŸ’¾ Auto-Guardado Inteligente
- Guarda sesiones automÃ¡ticamente cada N mensajes
- Logs en formato Markdown
- Historial completo de conversaciones

### ğŸ“ Modo Multi-LÃ­nea
- Pega cÃ³digo completo usando ` ``` `
- Preserva indentaciÃ³n y formato
- Perfecto para revisar/mejorar cÃ³digo

### ğŸ”„ Hot-Swapping de Modelos
- Cambia modelo en medio de conversaciÃ³n
- Preserva todo el historial
- Soporta cualquier modelo de Ollama

---

## ğŸš€ InstalaciÃ³n

### Requisitos Previos

```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Descargar un modelo
ollama pull llama3.2:latest
# o
ollama pull mistral:latest
```

### InstalaciÃ³n del Asistente

```bash
# Clonar repositorio
git clone https://github.com/[tu-usuario]/ai-assistant.git
cd ai-assistant

# Instalar dependencias
pip install ollama ddgs

# Ejecutar por primera vez (wizard de configuraciÃ³n)
python3 assistant.py
```

---

## ğŸ“– Uso

### Primera EjecuciÃ³n

Al ejecutar por primera vez, verÃ¡s el wizard de configuraciÃ³n:

```
ğŸ¤– CONFIGURACIÃ“N INICIAL - AI Assistant v7.82
============================================================

Bienvenido! Vamos a personalizar tu asistente.

Â¿CÃ³mo quieres llamar a tu asistente? [Assistant]: Jarvis
Â¿CuÃ¡l es tu nombre? [User]: Chris

Â¿QuÃ© rol debe tener tu asistente?
  1. Asistente tÃ©cnico/programaciÃ³n
  2. Asistente general/conversacional
  3. Tutor educativo
  4. Personalizado
Selecciona [1-4]: 1

âœ… ConfiguraciÃ³n guardada!
```

### Comandos Disponibles

| Comando | DescripciÃ³n |
|---------|-------------|
| `salir` / `exit` | Termina y guarda sesiÃ³n |
| `limpiar` | Borra memoria de conversaciÃ³n |
| `guardar` | Guarda sesiÃ³n manualmente |
| `modelo` | Cambia modelo (preserva historial) |
| `modelos` | Lista modelos disponibles |
| `buscar <query>` | BÃºsqueda web manual |
| ` ``` ` | Modo multi-lÃ­nea (terminar con ```) |
| `config` | Reconfigurar asistente |

---

## ğŸ® Ejemplos de Uso

### Ejemplo 1: BÃºsqueda Web Contextual

```
User> dame un cÃ³digo para escanear red con Python

[Assistant]: AquÃ­ tienes un script con Scapy...
```python
import scapy.all as scapy
...
```

User> busca como mejorarlo
ğŸŒ [Buscando]: 'busca como mejorarlo'...
   ğŸ’¡ Query mejorada: 'como mejorar cÃ³digo Python escanear red'
   ğŸ“‹ Con contexto conversacional
   Se encontraron 10 resultados

[Assistant]: BasÃ¡ndome en los resultados, aquÃ­ estÃ¡ el cÃ³digo mejorado:
```python
import scapy.all as scapy
import argparse
# [cÃ³digo mejorado completo]
```
âœ… Ahora entiende el contexto y genera soluciÃ³n!
```

### Ejemplo 2: Modo Multi-LÃ­nea

```
User> ```
ğŸ“ Modo multi-lÃ­nea activado...
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)
```
âœ“ CÃ³digo capturado (4 lÃ­neas)

User> optimiza esto
[Assistant]: AquÃ­ estÃ¡ optimizado con memoizaciÃ³n...
```

### Ejemplo 3: Cambio de Modelo

```
User> modelo

Modelo actual: llama3.2:latest

Modelos disponibles:
------------------------------------------------------------
â–º 1. llama3.2:latest                   (2.02 GB)
  2. mistral:latest                    (4.11 GB)
  3. codellama:latest                  (3.83 GB)
------------------------------------------------------------

Selecciona nuevo modelo [1-3]: 2
âœ“ Historial de conversaciÃ³n preservado
```

---

## âš™ï¸ ConfiguraciÃ³n

La configuraciÃ³n se guarda en `~/.ai_assistant/config.json`:

```json
{
  "assistant_name": "Assistant",
  "user_name": "User",
  "timezone": "America/Mexico_City",
  "logs_dir": "/home/user/.ai_assistant/logs",
  "max_messages_context": 20,
  "auto_save_interval": 10,
  "assistant_role": "asistente de IA",
  "user_expertise": "usuario tÃ©cnico",
  "language": "espaÃ±ol",
  "temperature": 0.7,
  "top_p": 0.9,
  "num_ctx": 8192,
  "num_predict": 800
}
```

### ParÃ¡metros Editables

- **assistant_name**: Nombre del asistente
- **user_name**: Tu nombre
- **logs_dir**: Directorio de logs
- **max_messages_context**: Ventana deslizante de contexto
- **auto_save_interval**: Auto-guardar cada N mensajes
- **assistant_role**: Rol del asistente
- **temperature**: Creatividad (0.0 = determinista, 1.0 = creativo)
- **top_p**: Diversidad de respuestas
- **num_ctx**: Tokens de contexto
- **num_predict**: Tokens mÃ¡ximos de respuesta

---

## ğŸ“‚ Estructura de Archivos

```
~/.ai_assistant/
â”œâ”€â”€ config.json          # ConfiguraciÃ³n personalizada
â””â”€â”€ logs/               # Logs de sesiones
    â”œâ”€â”€ session_20260104_120000.md
    â”œâ”€â”€ session_20260104_130000.md
    â””â”€â”€ ...
```

### Formato de Logs

```markdown
# SesiÃ³n de Assistant - 2026-01-04 12:00:00

**Usuario**: User  
**Modelo**: llama3.2:latest  
**Mensajes**: 15  
**Cambios de modelo**: 0

---

## ConversaciÃ³n

### > User (Mensaje #1)
Hola

### [Assistant] Respuesta #1
Â¡Hola! Â¿En quÃ© puedo ayudarte?
```

---

## ğŸ› Troubleshooting

### Error: "No se encontraron modelos instalados"

```bash
# SoluciÃ³n: Instalar un modelo
ollama pull llama3.2:latest
```

### Error: "Â¿EstÃ¡ Ollama corriendo?"

```bash
# SoluciÃ³n: Iniciar Ollama
ollama serve
# o verificar
ollama list
```

### Error: "ModuleNotFoundError: No module named 'ddgs'"

```bash
# SoluciÃ³n: Instalar dependencia
pip install ddgs
```

---

## ğŸ”§ Desarrollo

### Testing

```bash
# Verificar sintaxis
python3 -m py_compile assistant.py

# Ejecutar en modo debug
python3 assistant.py
```

### Contribuir

1. Fork el proyecto
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m 'AÃ±ade nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

---

## ğŸ“Š Changelog

### v7.82 (2026-01-04)
- âœ¨ Enriquecimiento inteligente de queries
- ğŸ¯ System prompt con acciÃ³n directa
- ğŸ’¡ Genera cÃ³digo basÃ¡ndose en bÃºsquedas
- ğŸ› Fix: Pasividad al recibir datos de bÃºsqueda

### v7.8 (2026-01-04)
- ğŸ” Enriquecimiento de queries vagas
- ğŸ’¡ "mejorarlo" â†’ "como mejorar [contexto]"
- ğŸ“‹ Feedback visual de query mejorada

### v7.75 (2026-01-04)
- ğŸŒ BÃºsqueda contextual universal
- ğŸ“‹ Contexto en TODAS las bÃºsquedas
- âœ¨ Nueva funciÃ³n `extraer_contexto_conversacional()`

### v7.7 (2026-01-04)
- ğŸ“… Contexto temporal mejorado
- â° HOY/AYER/MAÃ‘ANA explÃ­citos
- ğŸ› Fix: Variables de fecha correctamente expandidas

### v7.65 (2026-01-04)
- ğŸ” BÃºsqueda contextual (comando manual)
- ğŸ’¾ Auto-guardado cada 10 mensajes

### v7.6 (2026-01-03)
- ğŸ“ Modo multi-lÃ­nea con ` ``` `
- ğŸ”„ Hot-swapping de modelos
- ğŸ” Auto-bÃºsqueda inteligente

---

## ğŸ“„ Licencia

MIT License - Ãšsalo, modifÃ­calo, compÃ¡rtelo libremente.

---

## ğŸ™ Agradecimientos

- **Ollama Team** - Por la plataforma de LLM local
- **DuckDuckGo** - Por la API de bÃºsqueda
- Basado en **Jarvis** by Chris (@UPSLP)

---

## ğŸ“ Soporte

Â¿Problemas o sugerencias?

1. Abre un [issue en GitHub](https://github.com/[tu-usuario]/ai-assistant/issues)
2. Revisa la [documentaciÃ³n](#uso)
3. Consulta el [troubleshooting](#troubleshooting)

---

**ğŸš€ AI Assistant v7.82 - Tu asistente personalizable con IA local**
